import google.generativeai as genai
from typing import List, Optional
import os
import logging
from models.schemas import ChatMessage, ChatRequest
from services.go_api_client import get_go_api_client, format_episodes_for_context, format_materials_for_context

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


class GeminiChatService:
    """Gemini APIã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒ“ã‚¹"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.5-flash')

        # GoAPIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—
        self.go_api_client = get_go_api_client()

        # ä½œå®¶å‘ã‘ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.system_prompts = {
            "project": """
ã‚ãªãŸã¯ä½œå®¶ã®åŸ·ç­†ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æä¾›ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆåŸ·ç­†ä¸­ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰ã®å†…å®¹ã‚’å‚è€ƒã«ã—ã¦ã€åŸ·ç­†ã«å½¹ç«‹ã¤å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚
- ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹
- ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã®ç¢ºèª
- ãƒ—ãƒ­ãƒƒãƒˆã®å±•é–‹ã«é–¢ã™ã‚‹ææ¡ˆ
- æ–‡ç« ã®æ”¹å–„ææ¡ˆ
- æ—¢å­˜ã®æ–‡ç« ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
ãªã©ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ã—ã£ã‹ã‚Šã¨å‚ç…§ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
""",
            "dictionary": """
ã‚ãªãŸã¯ä½œå®¶å‘ã‘ã®è¡¨ç¾ãƒ»è¨€èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
- é©åˆ‡ãªè¨€è‘‰é¸ã³
- è¡¨ç¾ã®è±Šã‹ã•
- èªå½™ã®ææ¡ˆ
- æ–‡ç« ã®æ¨æ•²
- è¡¨ç¾æŠ€æ³•ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹
ãªã©ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚æ–‡å­¦çš„ã§ç¾ã—ã„è¡¨ç¾ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
""",
            "material": """
ã‚ãªãŸã¯è³‡æ–™ç ”ç©¶ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æä¾›ã•ã‚ŒãŸå‚è€ƒè³‡æ–™ã®å†…å®¹ã‚’åˆ†æã—ã€åŸ·ç­†ã«å½¹ç«‹ã¤æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®æŠ½å‡º
- é–¢é€£æƒ…å ±ã®è£œè¶³
- å‰µä½œã¸ã®å¿œç”¨æ–¹æ³•
- èƒŒæ™¯çŸ¥è­˜ã®èª¬æ˜
- è³‡æ–™é–“ã®é–¢é€£æ€§ã®æŒ‡æ‘˜
ãªã©ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚æä¾›ã•ã‚ŒãŸè³‡æ–™ã®å†…å®¹ã‚’ã—ã£ã‹ã‚Šã¨å‚ç…§ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        }

    async def generate_response(
        self,
        request: ChatRequest,
        chat_type: str = "general"
    ) -> ChatMessage:
        """ãƒãƒ£ãƒƒãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt = self.system_prompts.get(chat_type, "")

            # ä¼šè©±å±¥æ­´ã‚’æ§‹ç¯‰
            conversation_history = []

            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
            if system_prompt:
                conversation_history.append(f"ã‚·ã‚¹ãƒ†ãƒ : {system_prompt}")

            # GoAPIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
            content_context = ""
            content_not_found = []
            
            if chat_type == "project" and request.sources:
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ(episodes)ã‚’å–å¾—
                for source in request.sources:
                    # sourceã®å½¢å¼: "project:book_id:episode_id1,episode_id2,..." ã¾ãŸã¯ "book:book_id"
                    if source.startswith("project:") or source.startswith("book:"):
                        parts = source.split(":")
                        if len(parts) >= 3:
                            # project:book_id:episode_ids ã®å½¢å¼
                            book_id = parts[1]
                            episode_ids = parts[2].split(",") if parts[2] else []
                            
                            logger.info(f"ğŸ“– [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ] Fetching {len(episode_ids)} episodes for book: {book_id}")
                            try:
                                episodes = await self.go_api_client.get_episodes_by_ids(book_id, episode_ids)
                                
                                if episodes:
                                    content_context += format_episodes_for_context(episodes)
                                    logger.info(f"âœ… [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ] {len(episodes)} episodes loaded")
                                else:
                                    logger.warning(f"âš ï¸ [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ] No episodes found for IDs: {episode_ids}")
                                    content_not_found.append(f"project:{book_id}")
                            except Exception as e:
                                logger.error(f"âŒ [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ] Error fetching episodes: {e}")
                                content_not_found.append(f"project:{book_id}")
                        elif len(parts) >= 2:
                            # book:book_id ã®å½¢å¼ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
                            book_id = parts[1]
                            logger.warning(f"âš ï¸ [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ] Legacy format 'book:{book_id}' - no episode IDs provided")
                            content_not_found.append(f"book:{book_id}")
            
            elif chat_type == "material" and request.sources:
                # å‚è€ƒè³‡æ–™(materials)ã‚’å–å¾—
                logger.info(f"ğŸ” [è³‡æ–™] Processing sources: {request.sources}")
                for source in request.sources:
                    logger.info(f"ğŸ” [è³‡æ–™] Processing source: {source}")
                    # sourceã®å½¢å¼: "material:book_id:material_id1,material_id2,..." ã¾ãŸã¯ "book:book_id"
                    if source.startswith("material:") or source.startswith("book:"):
                        parts = source.split(":")
                        logger.info(f"ğŸ” [è³‡æ–™] Split parts: {parts}, length: {len(parts)}")
                        if len(parts) >= 3:
                            # material:book_id:material_ids ã®å½¢å¼
                            book_id = parts[1]
                            material_ids = parts[2].split(",") if parts[2] else []
                            
                            logger.info(f"ğŸ“š [è³‡æ–™] Fetching {len(material_ids)} materials for book: {book_id}")
                            try:
                                materials = await self.go_api_client.get_materials_by_ids(book_id, material_ids)
                                
                                if materials:
                                    content_context += format_materials_for_context(materials)
                                    logger.info(f"âœ… [è³‡æ–™] {len(materials)} materials loaded")
                                else:
                                    logger.warning(f"âš ï¸ [è³‡æ–™] No materials found for IDs: {material_ids}")
                                    content_not_found.append(f"material:{book_id}")
                            except Exception as e:
                                logger.error(f"âŒ [è³‡æ–™] Error fetching materials: {e}")
                                content_not_found.append(f"material:{book_id}")
                        elif len(parts) >= 2:
                            # book:book_id ã®å½¢å¼ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
                            book_id = parts[1]
                            logger.warning(f"âš ï¸ [è³‡æ–™] Legacy format 'book:{book_id}' - no material IDs provided")
                            content_not_found.append(f"book:{book_id}")

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if content_context:
                conversation_history.append(content_context)
            elif content_not_found:
                # ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®å¯¾å¿œ
                data_type_ja = "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ" if chat_type == "project" else "å‚è€ƒè³‡æ–™"
                logger.error(f"âŒ {data_type_ja}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {content_not_found}")
                import time
                return ChatMessage(
                    id=f"error-{int(time.time())}",
                    role="assistant", 
                    content=f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚æŒ‡å®šã•ã‚ŒãŸ{data_type_ja}ï¼ˆ{', '.join(content_not_found)}ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\n{data_type_ja}ãŒæ­£ã—ãç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                    ts=int(time.time() * 1000)
                )

            # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼å¯¾å¿œï¼‰
            if request.sources and not content_context and not content_not_found:
                source_info = f"å‚ç…§ã™ã‚‹ã‚½ãƒ¼ã‚¹: {', '.join(request.sources)}"
                conversation_history.append(source_info)

            # ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
            for msg in request.messages:
                role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg.role == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
                conversation_history.append(f"{role}: {msg.content}")

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµåˆ
            prompt = "\n".join(conversation_history)

            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯çœç•¥ï¼‰
            if len(prompt) > 1000:
                logger.info(
                    f"Generated prompt length: {len(prompt)} characters")
            else:
                logger.info(f"Generated prompt: {prompt}")

            # Gemini APIã‚’å‘¼ã³å‡ºã—
            logger.info(f"Generating response for chat_type: {chat_type}")

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
            if len(prompt) > 30000:  # 30KBåˆ¶é™
                logger.warning(
                    f"Prompt too long ({len(prompt)} chars), truncating...")
                prompt = prompt[:30000] + "\n\n[ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã„ãŸã‚çœç•¥ã•ã‚Œã¾ã—ãŸ]"

            # Gemini APIè¨­å®šã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–
            generation_config = genai.types.GenerationConfig(
                temperature=0.7,
                max_output_tokens=2000,  # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¢—ã‚„ã™
            )

            response = self.model.generate_content(
                prompt,
                generation_config=generation_config,
                stream=False
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¤œè¨¼
            response_text = ""
            
            if response.candidates:
                # å€™è£œãŒã‚ã‚‹å ´åˆã€æœ€åˆã®å€™è£œã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                candidate = response.candidates[0]
                
                if candidate.content and candidate.content.parts:
                    # å…¨ã¦ã®ãƒ‘ãƒ¼ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                    text_parts = []
                    for part in candidate.content.parts:
                        if hasattr(part, 'text') and part.text:
                            text_parts.append(part.text)
                    response_text = "".join(text_parts)
                    
                    # MAX_TOKENSã§åˆ‡ã‚Šæ¨ã¦ã‚‰ã‚ŒãŸå ´åˆã®ãƒ­ã‚°
                    if candidate.finish_reason == 2:  # MAX_TOKENS
                        logger.warning("Response was truncated due to max_tokens limit")
                elif candidate.finish_reason == 2:
                    # finish_reasonãŒMAX_TOKENSã®å ´åˆã€éƒ¨åˆ†çš„ã§ã‚‚contentãŒã‚ã‚Œã°å–å¾—ã‚’è©¦è¡Œ
                    try:
                        if candidate.content and hasattr(candidate.content, 'parts'):
                            text_parts = []
                            for part in candidate.content.parts:
                                if hasattr(part, 'text'):
                                    text_parts.append(str(part.text) if part.text else "")
                            response_text = "".join(text_parts)
                    except Exception as extract_error:
                        logger.error(f"Error extracting partial response: {extract_error}")
                        
                if not response_text.strip():
                    logger.warning("Response candidate has no content or parts")
                    response_text = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚AIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚"
            else:
                logger.warning("No candidates in response")
                response_text = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚AIã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"

            # ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not response_text.strip():
                logger.warning("Empty response text received")
                data_type_ja = "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ" if chat_type == "project" else "å‚è€ƒè³‡æ–™" if chat_type == "material" else "ãƒ‡ãƒ¼ã‚¿"
                if content_context:
                    response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚{data_type_ja}ã®å†…å®¹ã‚’å‚ç…§ã—ã¾ã—ãŸãŒã€é©åˆ‡ãªå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’å…·ä½“çš„ã«ã—ã¦ã„ãŸã ãã‹ã€åˆ¥ã®è¡¨ç¾ã§ãŠè©¦ã—ãã ã•ã„ã€‚"
                else:
                    response_text = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å‚ç…§ã™ã‚‹{data_type_ja}ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€å†…å®¹ãŒä¸ååˆ†ã§é©åˆ‡ãªå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚{data_type_ja}ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
            import time
            return ChatMessage(
                id=f"msg-{int(time.time())}-{abs(hash(response_text)) % 10000}",
                role="assistant",
                content=response_text,
                ts=int(time.time() * 1000)
            )

        except Exception as e:
            error_message = str(e)
            logger.error(f"Error generating response: {error_message}")

            # ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã«å¿œã˜ã¦ã‚ˆã‚Šå…·ä½“çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
            if "504" in error_message or "timeout" in error_message.lower():
                content = "ã‚µãƒ¼ãƒãƒ¼ã®å¿œç­”ã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çŸ­ãã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            elif "API_KEY" in error_message:
                content = "APIè¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
            elif "quota" in error_message.lower() or "limit" in error_message.lower():
                content = "APIåˆ©ç”¨åˆ¶é™ã«é”ã—ã¦ã„ã¾ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"
            else:
                content = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message[:100]}"

            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            import time
            return ChatMessage(
                id=f"error-{int(time.time())}",
                role="assistant",
                content=content,
                ts=int(time.time() * 1000)
            )

    async def search_dictionary(self, query: str) -> str:
        """è¾æ›¸æ¤œç´¢æ©Ÿèƒ½ï¼ˆGeminiã‚’ä½¿ç”¨ã—ã¦è©³ç´°ãªè§£èª¬ã‚’ç”Ÿæˆï¼‰"""
        try:
            prompt = f"""
ä»¥ä¸‹ã®å˜èªãƒ»è¡¨ç¾ã«ã¤ã„ã¦ã€ä½œå®¶å‘ã‘ã®è©³ç´°ãªè§£èª¬ã‚’ã—ã¦ãã ã•ã„ï¼š
ã€Œ{query}ã€

ä»¥ä¸‹ã®æƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
1. åŸºæœ¬çš„ãªæ„å‘³ãƒ»å®šç¾©
2. èªæºã‚„æˆã‚Šç«‹ã¡ï¼ˆåˆ†ã‹ã‚‹å ´åˆï¼‰
3. ä½¿ç”¨ä¾‹ãƒ»ç”¨ä¾‹
4. é¡èªãƒ»é¡ä¼¼è¡¨ç¾
5. æ–‡å­¦ä½œå“ã§ã®ä½¿ç”¨ä¾‹ï¼ˆã‚ã‚Œã°ï¼‰
6. ä½œå®¶ã¨ã—ã¦ã®åŠ¹æœçš„ãªä½¿ã„æ–¹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹

è©³ã—ãã€åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
"""
            response = self.model.generate_content(prompt)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¤œè¨¼
            response_text = ""
            if response.candidates:
                candidate = response.candidates[0]
                if candidate.content and candidate.content.parts:
                    text_parts = []
                    for part in candidate.content.parts:
                        if hasattr(part, 'text') and part.text:
                            text_parts.append(part.text)
                    response_text = "".join(text_parts)
                else:
                    response_text = f"ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿æŸ»ä¸­ã§ã™ã€‚è©³ç´°ãªæƒ…å ±ã¯å¾Œã»ã©æä¾›ã„ãŸã—ã¾ã™ã€‚"
            else:
                response_text = f"ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿æŸ»ä¸­ã§ã™ã€‚è©³ç´°ãªæƒ…å ±ã¯å¾Œã»ã©æä¾›ã„ãŸã—ã¾ã™ã€‚"
            
            return response_text if response_text.strip() else f"ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿æŸ»ä¸­ã§ã™ã€‚è©³ç´°ãªæƒ…å ±ã¯å¾Œã»ã©æä¾›ã„ãŸã—ã¾ã™ã€‚"

        except Exception as e:
            logger.error(f"Error in dictionary search: {e}")
            return f"ã€Œ{query}ã€ã«ã¤ã„ã¦èª¿æŸ»ä¸­ã§ã™ã€‚è©³ç´°ãªæƒ…å ±ã¯å¾Œã»ã©æä¾›ã„ãŸã—ã¾ã™ã€‚"


# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_gemini_service = None


def get_gemini_service() -> GeminiChatService:
    """Geminiã‚µãƒ¼ãƒ“ã‚¹ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _gemini_service
    if _gemini_service is None:
        import os
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is not set")
        _gemini_service = GeminiChatService(api_key=api_key)
    return _gemini_service
